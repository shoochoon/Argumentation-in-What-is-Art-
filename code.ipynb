{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de0f37b-40dd-4780-a2a4-c1136903c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic similarity\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# --- Cleaning ---\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[\"“”«»‘’]', '', text)\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)      # [1], [12]\n",
    "    text = re.sub(r'\\d+\\)', '', text)        # 1)\n",
    "    text = re.sub(r'[¹²³⁴⁵⁶⁷⁸⁹⁰]', '', text)  # superscripts\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --- Load SBERT model ---\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "similarity_scores = []\n",
    "\n",
    "with open('project-3-at-2025-05-19-09-43-1d74e9d7.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# --- Process each document ---\n",
    "for doc in data:\n",
    "    try:\n",
    "        results = doc.get('annotations', [{}])[0].get('result', [])\n",
    "\n",
    "        span_dict = {}\n",
    "        # Collect spans\n",
    "        for item in results:\n",
    "            if item.get('type') == 'labels' and 'value' in item:\n",
    "                span_id = item.get('id')\n",
    "                text = item['value'].get('text', '')\n",
    "                labels = item['value'].get('labels', [])\n",
    "                if span_id and text:\n",
    "                    span_dict[span_id] = {\n",
    "                        'text': clean_text(text),\n",
    "                        'labels': labels\n",
    "                    }\n",
    "\n",
    "        # Process relations\n",
    "        for item in results:\n",
    "            if item.get('type') == 'relation':\n",
    "                from_id = item.get('from_id')\n",
    "                to_id = item.get('to_id')\n",
    "                relation_labels = item.get('labels', [])\n",
    "                relation_label = relation_labels[0] if relation_labels else \"Unknown\"\n",
    "\n",
    "                if from_id in span_dict and to_id in span_dict:\n",
    "                    text1 = span_dict[from_id]['text']\n",
    "                    text2 = span_dict[to_id]['text']\n",
    "                    label1 = span_dict[from_id]['labels']\n",
    "                    label2 = span_dict[to_id]['labels']\n",
    "\n",
    "                    if len(text1.split()) < 3 or len(text2.split()) < 3:\n",
    "                        continue\n",
    "\n",
    "                    emb1 = model.encode(text1, convert_to_tensor=True)\n",
    "                    emb2 = model.encode(text2, convert_to_tensor=True)\n",
    "\n",
    "                    similarity = util.pytorch_cos_sim(emb1, emb2).item()\n",
    "\n",
    "                    similarity_scores.append({\n",
    "                        \"text1\": text1,\n",
    "                        \"text2\": text2,\n",
    "                        \"similarity\": round(similarity, 4),\n",
    "                        \"label1\": label1,\n",
    "                        \"label2\": label2,\n",
    "                        \"relation\": relation_label\n",
    "                    })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing doc {doc.get('id', 'unknown')}: {e}\")\n",
    "        continue\n",
    "\n",
    "# --- Save or inspect results ---\n",
    "result_df = pd.DataFrame(similarity_scores)\n",
    "print(result_df.head())\n",
    "\n",
    "result_df.to_csv(\"semantic_similarity_output.csv\", index=False)\n",
    "\n",
    "high_sim_df = result_df[result_df['similarity'] >= 0.8]\n",
    "thresholds = [0.8, 0.85]\n",
    "\n",
    "print(\"Number of pairs in each similarity group:\")\n",
    "for t in thresholds:\n",
    "    count = (result_df['similarity'] >= t).sum()\n",
    "    print(f\"Similarity >= {t}: {count} pairs\")\n",
    "\n",
    "# Optional: print pairs grouped by threshold with relation\n",
    "for t in thresholds:\n",
    "    print(f\"\\n--- Pairs with similarity >= {t} ---\")\n",
    "    filtered = result_df[result_df['similarity'] >= t]\n",
    "    if filtered.empty:\n",
    "        print(\"No pairs found.\")\n",
    "        continue\n",
    "\n",
    "    for idx, row in filtered.iterrows():\n",
    "        print(f\"Pair {idx}:\")\n",
    "        print(f\"Relation: {row['relation']}\")\n",
    "        print(f\"Text 1: {row['text1']}\")\n",
    "        print(f\"Label 1: {row['label1']}\")\n",
    "        print(f\"Text 2: {row['text2']}\")\n",
    "        print(f\"Label 2: {row['label2']}\")\n",
    "        print(f\"Similarity: {row['similarity']:.3f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "for relation, group in high_sim_df.groupby('relation'):\n",
    "    filename = f'high_similarity_pairs_relation_{relation}.csv'\n",
    "    group.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(group)} pairs with relation '{relation}' to {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da18f34-d4ef-4686-8da6-c053fd5097bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting chains of elaborations\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_elaboration_restatement_chains(df, min_length=3):\n",
    "    relevant_relations = ['Elaboration', 'Restatement']\n",
    "    df_filtered = df[df['relation'].isin(relevant_relations)].copy()\n",
    "\n",
    "    def parse_label(x):\n",
    "        if isinstance(x, list):\n",
    "            return x\n",
    "        elif isinstance(x, str):\n",
    "            return eval(x)  # assumes trusted input\n",
    "        return []\n",
    "\n",
    "    df_filtered['label2'] = df_filtered['label2'].apply(parse_label)\n",
    "\n",
    "    chains = defaultdict(list)\n",
    "    for _, row in df_filtered.iterrows():\n",
    "        if 'N' in row['label2']:\n",
    "            key = row['text2']  # nucleus\n",
    "            chains[key].append((row['relation'], row['text1'], row['label1']))\n",
    "\n",
    "    long_chains = {k: v for k, v in chains.items() if len(v) >= min_length}\n",
    "    return long_chains\n",
    "\n",
    "# ---- Extract chains ----\n",
    "chains = extract_elaboration_restatement_chains(result_df, min_length=3)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for nucleus, supports in chains.items():\n",
    "    print(f\"\\nNucleus:\\n{nucleus}\")\n",
    "    for rel, sat_text, sat_label in supports:\n",
    "        print(f\"  → {rel}: {sat_text} [Label: {sat_label}]\")\n",
    "\n",
    "for nucleus, supports in chains.items():\n",
    "    for rel, sat_text, sat_label in supports:\n",
    "        rows.append({\n",
    "            'Nucleus': nucleus,\n",
    "            'Relation': rel,\n",
    "            'Satellite Text': sat_text,\n",
    "            'Satellite Label': sat_label\n",
    "        })\n",
    "\n",
    "chains_df = pd.DataFrame(rows)\n",
    "print(len(chains_df))\n",
    "chains_df.to_csv(\"elaboration_chains.csv\", index=False)\n",
    "\n",
    "def load_chains_as_dict(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    chains = {}\n",
    "    for nucleus, group in df.groupby('Nucleus'):\n",
    "        satellites = []\n",
    "        for _, row in group.iterrows():\n",
    "            satellites.append((row['Relation'], row['Satellite Text'], row['Satellite Label']))\n",
    "        chains[nucleus] = satellites\n",
    "    return chains\n",
    "\n",
    "chains = load_chains_as_dict(\"elaboration_chains.csv\")\n",
    "\n",
    "texts_to_analyze = []\n",
    "\n",
    "for nucleus, satellites in chains.items():\n",
    "    block = f\"Nucleus:\\n{nucleus}\\n\"\n",
    "    for rel, sat_text, sat_label in satellites:\n",
    "        block += f\"→ {rel}: {sat_text} [Label: {sat_label}]\\n\"\n",
    "    texts_to_analyze.append(block.strip())\n",
    "\n",
    "api_input_text = \"\\n\\n---\\n\\n\".join(texts_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e53f00-ac5e-4ea7-b2d7-0cf5af417806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying fallacies using LLMs (initial stage using Claude, with stance 8 being the example)\n",
    "\n",
    "pip install anthropic\n",
    "import anthropic\n",
    "import os\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"key\" \n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "def process_with_claude(content, system_prompt):\n",
    "    print(\"Sending request to Claude API...\")\n",
    "\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-7-sonnet-20250219\",  # Correct model string\n",
    "            system=system_prompt,  # System prompt goes here\n",
    "            max_tokens=8000,\n",
    "            temperature=0.0,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": content}\n",
    "            ]\n",
    "        )\n",
    "        # Return the response content\n",
    "        return response.content[0].text\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Claude API: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "texts_to_analyze = []\n",
    "\n",
    "for nucleus, satellites in chains.items():\n",
    "    block = f\"Nucleus:\\n{nucleus}\\n\"\n",
    "    for rel, sat_text, sat_label in satellites:\n",
    "        block += f\"→ {rel}: {sat_text} [Label: {sat_label}]\\n\"\n",
    "    texts_to_analyze.append(block.strip())\n",
    "\n",
    "api_input_text = \"\\n\\n---\\n\\n\".join(texts_to_analyze)\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an expert in rhetorical theory and informal logic. \"\n",
    "    \"Analyze the following discourse chains for potential fallacies according to informal logic. \"\n",
    "    \"Meanwhile, pay attention to the rhetorical relations that are present in the chains that contain fallacies. \"\n",
    "    \"See if there are potential correlation or patterns. \"\n",
    "    \"Return your findings with references to the relevant text.\"\n",
    ")\n",
    "\n",
    "fallacy_analysis = process_with_claude(api_input_text, system_prompt)\n",
    "print(fallacy_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc12dff-0d3f-4913-8dd8-5826f77ac01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation step using both Claude and Gemini; code for google colab\n",
    "pip install -q -U google-generativeai\n",
    "\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "\n",
    "def process_with_gemini(content):\n",
    "    model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "\n",
    "    user_prompt = (\n",
    "        \"You are an expert in rhetorical theory and informal logic. \"\n",
    "        \"Analyze the following discourse chains for potential fallacies according to informal logic. \"\n",
    "        \"Meanwhile, pay attention to the rhetorical relations that are present in the chains that contain fallacies. \"\n",
    "        \"See if there are potential correlation or patterns. \"\n",
    "        \"Return your findings with references to the relevant text.\"\n",
    "        f\"{content}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            user_prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                max_output_tokens=4000, # Max output tokens\n",
    "                temperature=0.0,       # Lower temperature for less creativity, more factual\n",
    "            )\n",
    "        )\n",
    "        # Gemini's response structure can vary, content[0].text is common for simple text\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Gemini API: {str(e)}\")\n",
    "        # If you want to re-raise the exception to stop execution:\n",
    "        # raise\n",
    "        return None # Return None if an error occurs\n",
    "\n",
    "texts_to_analyze = []\n",
    "\n",
    "for nucleus, satellites in chains.items():\n",
    "    block = f\"Nucleus:\\n{nucleus}\\n\"\n",
    "    for rel, sat_text, sat_label in satellites:\n",
    "        block += f\"→ {rel}: {sat_text} [Label: {sat_label}]\\n\"\n",
    "    texts_to_analyze.append(block.strip())\n",
    "\n",
    "api_input_text = \"\\n\\n---\\n\\n\".join(texts_to_analyze)\n",
    "\n",
    "# --- Run the analysis twice ---\n",
    "\n",
    "print(\"\\n--- FIRST RUN OF GEMINI ANALYSIS ---\")\n",
    "fallacy_analysis_run1 = process_with_gemini(api_input_text)\n",
    "if fallacy_analysis_run1:\n",
    "    print(fallacy_analysis_run1)\n",
    "else:\n",
    "    print(\"First Gemini run failed.\")\n",
    "\n",
    "print(\"\\n--- SECOND RUN OF GEMINI ANALYSIS ---\")\n",
    "fallacy_analysis_run2 = process_with_gemini(api_input_text)\n",
    "if fallacy_analysis_run2:\n",
    "    print(fallacy_analysis_run2)\n",
    "else:\n",
    "    print(\"Second Gemini run failed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
